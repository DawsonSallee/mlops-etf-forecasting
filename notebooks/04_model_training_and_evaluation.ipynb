{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dca4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import shap\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mlflow.set_tracking_uri(\"../mlruns\")\n",
    "\n",
    "# Load your processed data\n",
    "DATA_PATH = '../data/processed/etf_features.parquet'\n",
    "data = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb7dc1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2380\n",
      "Test set size: 908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:c:/Users/dawso/Dev/Personal/AIGrind/mlops-etf-forecasting/notebooks/../mlruns/585456644078602194', creation_time=1756042193765, experiment_id='585456644078602194', last_update_time=1756042193765, lifecycle_stage='active', name='ETF_Trend_Prediction', tags={}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the chronological split point\n",
    "# For example, use data up to the end of 2021 for training, and 2022 onwards for testing.\n",
    "split_date = '2022-01-01'\n",
    "X_train, X_test = X.loc[:split_date], X.loc[split_date:]\n",
    "y_train, y_test = y.loc[:split_date], y.loc[split_date:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "mlflow.set_experiment(\"ETF_Trend_Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b6db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression F1 Score: 0.6968\n",
      "Random Forest F1 Score: 0.6299\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_Baseline\"):\n",
    "    model_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    y_pred_lr = model_lr.predict(X_test)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred_lr))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred_lr))\n",
    "    print(f\"Logistic Regression F1 Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "\n",
    "# Train Random Forest\n",
    "with mlflow.start_run(run_name=\"RandomForest_Baseline\"):\n",
    "    model_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred_rf))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred_rf))\n",
    "    print(f\"Random Forest F1 Score: {f1_score(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05dee337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Use TimeSeriesSplit for robust cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=tscv, scoring='f1', n_jobs=-1).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b1b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-24 10:00:58,572] A new study created in memory with name: no-name-db62bc66-a317-43d5-ad3d-b6dd52805eba\n",
      "[I 2025-08-24 10:00:58,915] Trial 0 finished with value: 0.4946430502993634 and parameters: {'n_estimators': 285, 'max_depth': 6, 'learning_rate': 0.20483613466782621, 'subsample': 0.8076561908348419, 'colsample_bytree': 0.7164187067291982, 'gamma': 2.2429486147010946}. Best is trial 0 with value: 0.4946430502993634.\n",
      "[I 2025-08-24 10:00:59,327] Trial 1 finished with value: 0.5259837898068112 and parameters: {'n_estimators': 784, 'max_depth': 6, 'learning_rate': 0.2099252816674043, 'subsample': 0.829390110636925, 'colsample_bytree': 0.7474895144163843, 'gamma': 4.712240032937681}. Best is trial 1 with value: 0.5259837898068112.\n",
      "[I 2025-08-24 10:00:59,801] Trial 2 finished with value: 0.48336231314253403 and parameters: {'n_estimators': 792, 'max_depth': 4, 'learning_rate': 0.069458556351941, 'subsample': 0.9414385898329014, 'colsample_bytree': 0.8296194198135133, 'gamma': 3.511706136713248}. Best is trial 1 with value: 0.5259837898068112.\n",
      "[I 2025-08-24 10:01:00,565] Trial 3 finished with value: 0.5535871014840806 and parameters: {'n_estimators': 531, 'max_depth': 6, 'learning_rate': 0.12246532459247789, 'subsample': 0.5258088773164978, 'colsample_bytree': 0.8750029864978686, 'gamma': 1.5373466963415665}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:00,873] Trial 4 finished with value: 0.5023385571567626 and parameters: {'n_estimators': 568, 'max_depth': 6, 'learning_rate': 0.29840285509580794, 'subsample': 0.9139761847946042, 'colsample_bytree': 0.5825333306155951, 'gamma': 2.1595685989687654}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:01,242] Trial 5 finished with value: 0.4905336731008684 and parameters: {'n_estimators': 624, 'max_depth': 6, 'learning_rate': 0.2929244618006749, 'subsample': 0.8255032269544347, 'colsample_bytree': 0.8497853144739287, 'gamma': 3.599336208594713}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:01,475] Trial 6 finished with value: 0.5452542890782327 and parameters: {'n_estimators': 105, 'max_depth': 10, 'learning_rate': 0.10911683954365517, 'subsample': 0.5352497428315914, 'colsample_bytree': 0.913674632422774, 'gamma': 4.899306287540888}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:01,729] Trial 7 finished with value: 0.5324345296895217 and parameters: {'n_estimators': 421, 'max_depth': 7, 'learning_rate': 0.24321247612255292, 'subsample': 0.7484296951447894, 'colsample_bytree': 0.6061734168113153, 'gamma': 4.4159112362590776}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:02,100] Trial 8 finished with value: 0.5459849840091427 and parameters: {'n_estimators': 461, 'max_depth': 3, 'learning_rate': 0.012832682231832579, 'subsample': 0.6052586293726506, 'colsample_bytree': 0.5610026699067023, 'gamma': 4.616732302417519}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:02,394] Trial 9 finished with value: 0.4870144939646776 and parameters: {'n_estimators': 615, 'max_depth': 3, 'learning_rate': 0.24409729806207095, 'subsample': 0.9609598634437637, 'colsample_bytree': 0.7845217428102186, 'gamma': 3.0898960637063992}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:03,600] Trial 10 finished with value: 0.545730059154476 and parameters: {'n_estimators': 918, 'max_depth': 8, 'learning_rate': 0.14098472028331563, 'subsample': 0.6618490975799325, 'colsample_bytree': 0.9805651361956014, 'gamma': 0.5889363132352547}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:04,188] Trial 11 finished with value: 0.5485527104803238 and parameters: {'n_estimators': 407, 'max_depth': 4, 'learning_rate': 0.013945898863472667, 'subsample': 0.5013579267453155, 'colsample_bytree': 0.5054277425708478, 'gamma': 0.8161865246597859}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:04,656] Trial 12 finished with value: 0.5534708413776709 and parameters: {'n_estimators': 315, 'max_depth': 4, 'learning_rate': 0.015068089699814909, 'subsample': 0.5043995437022977, 'colsample_bytree': 0.5026130264842525, 'gamma': 0.8910302040064749}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:05,218] Trial 13 finished with value: 0.5197225538600017 and parameters: {'n_estimators': 237, 'max_depth': 5, 'learning_rate': 0.06913995491211464, 'subsample': 0.5987118507473406, 'colsample_bytree': 0.6410958980646401, 'gamma': 1.267567844444332}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:05,976] Trial 14 finished with value: 0.5183707234928404 and parameters: {'n_estimators': 257, 'max_depth': 8, 'learning_rate': 0.06866017631068484, 'subsample': 0.6647681274638331, 'colsample_bytree': 0.6804216635093053, 'gamma': 1.4564280249047896}. Best is trial 3 with value: 0.5535871014840806.\n",
      "[I 2025-08-24 10:01:06,627] Trial 15 finished with value: 0.5549392233136959 and parameters: {'n_estimators': 334, 'max_depth': 4, 'learning_rate': 0.12069169979376435, 'subsample': 0.5638315259720006, 'colsample_bytree': 0.9129736791858952, 'gamma': 0.07215240763039399}. Best is trial 15 with value: 0.5549392233136959.\n",
      "[I 2025-08-24 10:01:07,199] Trial 16 finished with value: 0.5496142340268033 and parameters: {'n_estimators': 162, 'max_depth': 5, 'learning_rate': 0.148711669295539, 'subsample': 0.5759955667193064, 'colsample_bytree': 0.9911973388551267, 'gamma': 0.21704519495309518}. Best is trial 15 with value: 0.5549392233136959.\n",
      "[I 2025-08-24 10:01:07,902] Trial 17 finished with value: 0.5450815975388127 and parameters: {'n_estimators': 501, 'max_depth': 8, 'learning_rate': 0.113398975863733, 'subsample': 0.6719504386846856, 'colsample_bytree': 0.9137994739842544, 'gamma': 1.7822092621559844}. Best is trial 15 with value: 0.5549392233136959.\n",
      "[I 2025-08-24 10:01:09,469] Trial 18 finished with value: 0.5638428027931033 and parameters: {'n_estimators': 723, 'max_depth': 5, 'learning_rate': 0.18213665934952214, 'subsample': 0.7132319346346189, 'colsample_bytree': 0.9109983561136107, 'gamma': 0.005679320458254922}. Best is trial 18 with value: 0.5638428027931033.\n",
      "[I 2025-08-24 10:01:10,940] Trial 19 finished with value: 0.5540368731313325 and parameters: {'n_estimators': 725, 'max_depth': 5, 'learning_rate': 0.18095952721025765, 'subsample': 0.7222115718421017, 'colsample_bytree': 0.9196093365471614, 'gamma': 0.040263607293668624}. Best is trial 18 with value: 0.5638428027931033.\n",
      "[I 2025-08-24 10:01:11,700] Trial 20 finished with value: 0.49295904885972686 and parameters: {'n_estimators': 933, 'max_depth': 3, 'learning_rate': 0.1715729348159072, 'subsample': 0.891706480403079, 'colsample_bytree': 0.798480360022393, 'gamma': 0.4464245354037366}. Best is trial 18 with value: 0.5638428027931033.\n",
      "[I 2025-08-24 10:01:13,153] Trial 21 finished with value: 0.5629800888144721 and parameters: {'n_estimators': 726, 'max_depth': 5, 'learning_rate': 0.18029092719886966, 'subsample': 0.7381972779091264, 'colsample_bytree': 0.9410862955645047, 'gamma': 0.05115062364768566}. Best is trial 18 with value: 0.5638428027931033.\n",
      "[I 2025-08-24 10:01:14,366] Trial 22 finished with value: 0.5597126668892919 and parameters: {'n_estimators': 685, 'max_depth': 4, 'learning_rate': 0.1979989412552893, 'subsample': 0.698648911291542, 'colsample_bytree': 0.9492772278837356, 'gamma': 0.023063207677446962}. Best is trial 18 with value: 0.5638428027931033.\n",
      "[I 2025-08-24 10:01:15,017] Trial 23 finished with value: 0.5699844776576182 and parameters: {'n_estimators': 691, 'max_depth': 5, 'learning_rate': 0.2099188440773813, 'subsample': 0.7084236219872019, 'colsample_bytree': 0.9562967435380642, 'gamma': 1.0029757179018168}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:15,692] Trial 24 finished with value: 0.5300504722882021 and parameters: {'n_estimators': 853, 'max_depth': 5, 'learning_rate': 0.2361788829919203, 'subsample': 0.7784367919242708, 'colsample_bytree': 0.9602314408873025, 'gamma': 1.0358652538745434}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:16,469] Trial 25 finished with value: 0.5434063882754964 and parameters: {'n_estimators': 707, 'max_depth': 7, 'learning_rate': 0.22431597018914656, 'subsample': 0.7572200110013699, 'colsample_bytree': 0.8584309292608362, 'gamma': 0.5569217445708113}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:17,405] Trial 26 finished with value: 0.5581402874652295 and parameters: {'n_estimators': 997, 'max_depth': 5, 'learning_rate': 0.26242902223489345, 'subsample': 0.6290314151289711, 'colsample_bytree': 0.9979581112002434, 'gamma': 0.4798790773218258}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:18,091] Trial 27 finished with value: 0.5271281683982164 and parameters: {'n_estimators': 809, 'max_depth': 7, 'learning_rate': 0.16141655674652766, 'subsample': 0.7152939260270462, 'colsample_bytree': 0.8801994264413497, 'gamma': 1.8315746148084924}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:18,820] Trial 28 finished with value: 0.5174685810475051 and parameters: {'n_estimators': 646, 'max_depth': 10, 'learning_rate': 0.1894431334407573, 'subsample': 0.7925210141951076, 'colsample_bytree': 0.9393876152982226, 'gamma': 1.107297677613885}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:19,288] Trial 29 finished with value: 0.5016615331109604 and parameters: {'n_estimators': 747, 'max_depth': 5, 'learning_rate': 0.26670720690043004, 'subsample': 0.835827532294739, 'colsample_bytree': 0.8058006212925841, 'gamma': 2.5938886596489295}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:20,037] Trial 30 finished with value: 0.5365564219730142 and parameters: {'n_estimators': 871, 'max_depth': 6, 'learning_rate': 0.21015659479508023, 'subsample': 0.7468036353475921, 'colsample_bytree': 0.7417744299915572, 'gamma': 0.7111982266244269}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:20,896] Trial 31 finished with value: 0.5575806609870988 and parameters: {'n_estimators': 674, 'max_depth': 4, 'learning_rate': 0.19404474583826353, 'subsample': 0.6969362762774631, 'colsample_bytree': 0.952163043341657, 'gamma': 0.29857186691502946}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:21,925] Trial 32 finished with value: 0.5494846886731148 and parameters: {'n_estimators': 604, 'max_depth': 4, 'learning_rate': 0.2085168932481032, 'subsample': 0.7085226787908115, 'colsample_bytree': 0.8916390886334362, 'gamma': 0.003371974774720711}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:22,915] Trial 33 finished with value: 0.547369082051041 and parameters: {'n_estimators': 765, 'max_depth': 5, 'learning_rate': 0.1687901349548808, 'subsample': 0.6400387680752933, 'colsample_bytree': 0.9530549762609188, 'gamma': 0.46414062292161784}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:23,682] Trial 34 finished with value: 0.5226525260394159 and parameters: {'n_estimators': 682, 'max_depth': 3, 'learning_rate': 0.22176208094032035, 'subsample': 0.8580846122995298, 'colsample_bytree': 0.9691618728680362, 'gamma': 0.2783289667408865}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:24,531] Trial 35 finished with value: 0.5352635433844283 and parameters: {'n_estimators': 816, 'max_depth': 4, 'learning_rate': 0.13876735224568068, 'subsample': 0.6793640535198583, 'colsample_bytree': 0.8371257188144334, 'gamma': 0.8283514159314811}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:25,114] Trial 36 finished with value: 0.5371495222515045 and parameters: {'n_estimators': 566, 'max_depth': 6, 'learning_rate': 0.1946084458140562, 'subsample': 0.7728050741671207, 'colsample_bytree': 0.9383240671508568, 'gamma': 1.2768109444572417}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:25,737] Trial 37 finished with value: 0.5334336585936493 and parameters: {'n_estimators': 740, 'max_depth': 4, 'learning_rate': 0.18485774508093009, 'subsample': 0.734445376428516, 'colsample_bytree': 0.9988692187705792, 'gamma': 2.614831306419635}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:26,156] Trial 38 finished with value: 0.5399207151041135 and parameters: {'n_estimators': 524, 'max_depth': 5, 'learning_rate': 0.2617130921772479, 'subsample': 0.8085276010665046, 'colsample_bytree': 0.8884381515351186, 'gamma': 2.0969950294946234}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:26,573] Trial 39 finished with value: 0.5260977778697551 and parameters: {'n_estimators': 664, 'max_depth': 6, 'learning_rate': 0.15492673335461987, 'subsample': 0.7017569885683753, 'colsample_bytree': 0.8579737049301238, 'gamma': 4.0269646365195975}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:27,252] Trial 40 finished with value: 0.5109255670096455 and parameters: {'n_estimators': 598, 'max_depth': 3, 'learning_rate': 0.09388110660922773, 'subsample': 0.6387430597669516, 'colsample_bytree': 0.7700748195138771, 'gamma': 0.2565447450064107}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:28,109] Trial 41 finished with value: 0.5683753501009732 and parameters: {'n_estimators': 842, 'max_depth': 5, 'learning_rate': 0.263625408232836, 'subsample': 0.6293558740219419, 'colsample_bytree': 0.9765426262526109, 'gamma': 0.4809873712292462}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:29,527] Trial 42 finished with value: 0.5581218105214159 and parameters: {'n_estimators': 857, 'max_depth': 6, 'learning_rate': 0.22620276266257733, 'subsample': 0.6856039917360424, 'colsample_bytree': 0.9179228409200972, 'gamma': 0.055083941286345084}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:30,257] Trial 43 finished with value: 0.5665979894393871 and parameters: {'n_estimators': 783, 'max_depth': 5, 'learning_rate': 0.2863850329892666, 'subsample': 0.6157738807602393, 'colsample_bytree': 0.9743253417432736, 'gamma': 0.6641543488515025}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:31,102] Trial 44 finished with value: 0.5552525818086947 and parameters: {'n_estimators': 912, 'max_depth': 7, 'learning_rate': 0.2854187849861893, 'subsample': 0.5861604829117695, 'colsample_bytree': 0.9766876560725581, 'gamma': 0.7345989301694906}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:31,794] Trial 45 finished with value: 0.5458224431816148 and parameters: {'n_estimators': 783, 'max_depth': 5, 'learning_rate': 0.28578948720004627, 'subsample': 0.6130563805341924, 'colsample_bytree': 0.9363086940982545, 'gamma': 1.1293549561413285}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:32,454] Trial 46 finished with value: 0.5529041618981435 and parameters: {'n_estimators': 825, 'max_depth': 6, 'learning_rate': 0.27467644957411325, 'subsample': 0.5610033581146519, 'colsample_bytree': 0.8987916820567663, 'gamma': 1.532673105456727}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:33,049] Trial 47 finished with value: 0.5583092815722472 and parameters: {'n_estimators': 777, 'max_depth': 9, 'learning_rate': 0.24984599112500808, 'subsample': 0.9960822191774177, 'colsample_bytree': 0.9730767091749236, 'gamma': 0.9705399309945266}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:33,869] Trial 48 finished with value: 0.5539020819755233 and parameters: {'n_estimators': 964, 'max_depth': 5, 'learning_rate': 0.2514348967333009, 'subsample': 0.6545952430675994, 'colsample_bytree': 0.8263050651020352, 'gamma': 0.6382495940057185}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:34,561] Trial 49 finished with value: 0.5687059997044898 and parameters: {'n_estimators': 887, 'max_depth': 6, 'learning_rate': 0.2930344044317715, 'subsample': 0.5390825122315599, 'colsample_bytree': 0.9776775042400455, 'gamma': 3.1709366684053197}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:35,196] Trial 50 finished with value: 0.5254819027733729 and parameters: {'n_estimators': 908, 'max_depth': 6, 'learning_rate': 0.29894101904764014, 'subsample': 0.5344159262268307, 'colsample_bytree': 0.9840375359596856, 'gamma': 3.0911834409404775}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:35,801] Trial 51 finished with value: 0.544048022707839 and parameters: {'n_estimators': 861, 'max_depth': 6, 'learning_rate': 0.27131469122684715, 'subsample': 0.5525781621280029, 'colsample_bytree': 0.9279090079674839, 'gamma': 3.3941567024710886}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:36,299] Trial 52 finished with value: 0.5213077138368247 and parameters: {'n_estimators': 730, 'max_depth': 5, 'learning_rate': 0.28137422092983855, 'subsample': 0.6208741129043281, 'colsample_bytree': 0.9690907784114631, 'gamma': 3.7078490022467907}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:36,943] Trial 53 finished with value: 0.5475302749876005 and parameters: {'n_estimators': 889, 'max_depth': 5, 'learning_rate': 0.23722660243677873, 'subsample': 0.5184039008391231, 'colsample_bytree': 0.9012114027839253, 'gamma': 2.9512918565243806}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:37,483] Trial 54 finished with value: 0.5401600375095246 and parameters: {'n_estimators': 835, 'max_depth': 7, 'learning_rate': 0.2998093813330126, 'subsample': 0.5980628549887614, 'colsample_bytree': 0.701439374310903, 'gamma': 2.382659889843324}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:38,325] Trial 55 finished with value: 0.5498397723607807 and parameters: {'n_estimators': 791, 'max_depth': 5, 'learning_rate': 0.254293660977127, 'subsample': 0.7316799584035661, 'colsample_bytree': 0.9997595101279529, 'gamma': 0.31251396403386655}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:39,255] Trial 56 finished with value: 0.54026013986014 and parameters: {'n_estimators': 706, 'max_depth': 4, 'learning_rate': 0.1716865572508025, 'subsample': 0.5837353598339869, 'colsample_bytree': 0.871820523331733, 'gamma': 0.3904303777555511}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:40,161] Trial 57 finished with value: 0.5493447363360617 and parameters: {'n_estimators': 963, 'max_depth': 6, 'learning_rate': 0.21574237343883665, 'subsample': 0.6543260067755359, 'colsample_bytree': 0.9579911045835069, 'gamma': 1.3167512166164321}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:40,739] Trial 58 finished with value: 0.5467531331640775 and parameters: {'n_estimators': 646, 'max_depth': 5, 'learning_rate': 0.13909273366703337, 'subsample': 0.7604235928493503, 'colsample_bytree': 0.9314030919781198, 'gamma': 2.8537226712455253}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:41,477] Trial 59 finished with value: 0.5322071120022521 and parameters: {'n_estimators': 767, 'max_depth': 4, 'learning_rate': 0.2896262022155868, 'subsample': 0.6674340969537715, 'colsample_bytree': 0.9819346785817964, 'gamma': 0.605319453305394}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:42,324] Trial 60 finished with value: 0.5388498976183754 and parameters: {'n_estimators': 944, 'max_depth': 5, 'learning_rate': 0.27759551232927315, 'subsample': 0.5502435541306923, 'colsample_bytree': 0.9468667567866087, 'gamma': 1.7420662336702724}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:43,307] Trial 61 finished with value: 0.5425910881038621 and parameters: {'n_estimators': 701, 'max_depth': 4, 'learning_rate': 0.20075815734237382, 'subsample': 0.7354274862467645, 'colsample_bytree': 0.9593028788223799, 'gamma': 0.2137475450464919}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:44,470] Trial 62 finished with value: 0.5472482677786679 and parameters: {'n_estimators': 643, 'max_depth': 5, 'learning_rate': 0.18006948680356183, 'subsample': 0.6929993401859945, 'colsample_bytree': 0.9072681850458127, 'gamma': 0.15730833414927994}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:46,402] Trial 63 finished with value: 0.5464122707954244 and parameters: {'n_estimators': 745, 'max_depth': 6, 'learning_rate': 0.16135867373171053, 'subsample': 0.7112856090945523, 'colsample_bytree': 0.9218042290513709, 'gamma': 0.014715873378035704}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:47,289] Trial 64 finished with value: 0.5429965939417191 and parameters: {'n_estimators': 816, 'max_depth': 4, 'learning_rate': 0.23414130799591923, 'subsample': 0.6470132179296268, 'colsample_bytree': 0.9450618058435272, 'gamma': 0.45567126852801243}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:48,057] Trial 65 finished with value: 0.4908211781979893 and parameters: {'n_estimators': 691, 'max_depth': 5, 'learning_rate': 0.20041414023581375, 'subsample': 0.7880578525976407, 'colsample_bytree': 0.9670454120803236, 'gamma': 0.8450861915548527}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:48,614] Trial 66 finished with value: 0.5291152267485681 and parameters: {'n_estimators': 587, 'max_depth': 3, 'learning_rate': 0.25870737671915267, 'subsample': 0.6854076936347027, 'colsample_bytree': 0.644750036981962, 'gamma': 0.5738177782196334}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:49,807] Trial 67 finished with value: 0.5405253154821454 and parameters: {'n_estimators': 627, 'max_depth': 4, 'learning_rate': 0.18012755040618914, 'subsample': 0.7691704866331067, 'colsample_bytree': 0.989091138280658, 'gamma': 0.14057526705838885}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:50,373] Trial 68 finished with value: 0.5415462579305275 and parameters: {'n_estimators': 894, 'max_depth': 6, 'learning_rate': 0.2118798554034548, 'subsample': 0.6284471581921034, 'colsample_bytree': 0.8697430546164993, 'gamma': 4.060855662362092}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:52,166] Trial 69 finished with value: 0.5126131049411378 and parameters: {'n_estimators': 800, 'max_depth': 5, 'learning_rate': 0.034372852783777316, 'subsample': 0.7488149031602086, 'colsample_bytree': 0.5512827028795027, 'gamma': 0.3620382699972462}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:53,107] Trial 70 finished with value: 0.5268524103750372 and parameters: {'n_estimators': 717, 'max_depth': 4, 'learning_rate': 0.13000150317403772, 'subsample': 0.7188712971868175, 'colsample_bytree': 0.9458348131089799, 'gamma': 0.8738580740735319}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:53,767] Trial 71 finished with value: 0.5511959402781998 and parameters: {'n_estimators': 774, 'max_depth': 7, 'learning_rate': 0.24869705723972882, 'subsample': 0.9556635300056298, 'colsample_bytree': 0.9788822879905104, 'gamma': 1.0729958026761564}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:54,367] Trial 72 finished with value: 0.530123900465823 and parameters: {'n_estimators': 756, 'max_depth': 5, 'learning_rate': 0.2645398119578345, 'subsample': 0.9737471571323653, 'colsample_bytree': 0.9661979229003018, 'gamma': 0.971350105100138}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:55,346] Trial 73 finished with value: 0.5453341929655086 and parameters: {'n_estimators': 835, 'max_depth': 9, 'learning_rate': 0.24621991800153792, 'subsample': 0.6750645961921355, 'colsample_bytree': 0.926884698182174, 'gamma': 0.7378039034788362}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:56,539] Trial 74 finished with value: 0.5166796368959617 and parameters: {'n_estimators': 662, 'max_depth': 9, 'learning_rate': 0.22585870606859876, 'subsample': 0.9214519827915573, 'colsample_bytree': 0.9846408626958371, 'gamma': 0.16231028345763365}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:57,230] Trial 75 finished with value: 0.5549903010381569 and parameters: {'n_estimators': 877, 'max_depth': 10, 'learning_rate': 0.29385010108166676, 'subsample': 0.8740889700169511, 'colsample_bytree': 0.9603951237950992, 'gamma': 2.042939967733556}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:58,180] Trial 76 finished with value: 0.5059692053027758 and parameters: {'n_estimators': 722, 'max_depth': 9, 'learning_rate': 0.23341851558287968, 'subsample': 0.7353272893470731, 'colsample_bytree': 0.9143945576065867, 'gamma': 0.511775586342993}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:58,996] Trial 77 finished with value: 0.5585215313492686 and parameters: {'n_estimators': 792, 'max_depth': 8, 'learning_rate': 0.2708320067149926, 'subsample': 0.6016073087865035, 'colsample_bytree': 0.9388919010038433, 'gamma': 1.3601782394075057}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:01:59,741] Trial 78 finished with value: 0.5485022726299686 and parameters: {'n_estimators': 840, 'max_depth': 6, 'learning_rate': 0.26991048188001315, 'subsample': 0.6093258590311104, 'colsample_bytree': 0.8902120645970953, 'gamma': 1.3869680227934191}. Best is trial 23 with value: 0.5699844776576182.\n",
      "[I 2025-08-24 10:02:00,527] Trial 79 finished with value: 0.5673325328379372 and parameters: {'n_estimators': 459, 'max_depth': 8, 'learning_rate': 0.1893679814384237, 'subsample': 0.5767639646684128, 'colsample_bytree': 0.9389693234150269, 'gamma': 1.2165736126604312}. Best is trial 23 with value: 0.5699844776576182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Params: {'n_estimators': 691, 'max_depth': 5, 'learning_rate': 0.2099188440773813, 'subsample': 0.7084236219872019, 'colsample_bytree': 0.9562967435380642, 'gamma': 1.0029757179018168}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 10:02:01 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Tuned XGBoost F1 Score: 0.6090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dawso\\Dev\\Personal\\AIGrind\\mlops-etf-forecasting\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [10:02:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2025/08/24 10:02:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHAP analysis complete and plot logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "# Run the study to find the best params\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=125) \n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best XGBoost Params:\", best_params)\n",
    "\n",
    "# Train the final XGBoost model with the best parameters and log to MLflow\n",
    "with mlflow.start_run(run_name=\"XGBoost_Tuned_Champion\") as run:\n",
    "    final_xgb_model = xgb.XGBClassifier(**best_params, random_state=42)\n",
    "    final_xgb_model.fit(X_train, y_train)\n",
    "    y_pred_xgb = final_xgb_model.predict(X_test)\n",
    "    y_pred_proba_xgb = final_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred_xgb)\n",
    "    print(f\"Final Tuned XGBoost F1 Score: {f1:.4f}\")\n",
    "\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred_xgb))\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, y_pred_proba_xgb))\n",
    "\n",
    "    mlflow.xgboost.log_model(final_xgb_model, \"xgb-model\")\n",
    "    champion_run_id = run.info.run_id # Capture run ID\n",
    "\n",
    "    # --- SHAP Plot Generation and Logging (Move these lines here) ---\n",
    "    print(\"\\nSHAP analysis complete and plot logged to MLflow.\")\n",
    "\n",
    "    # 1. Create a SHAP Explainer\n",
    "    explainer = shap.TreeExplainer(final_xgb_model)\n",
    "    shap_values = explainer.shap_values(X_test) # Or X_train, depending on what you want to explain\n",
    "\n",
    "    # 2. Generate and save the SHAP summary plot to a temporary file\n",
    "    # Ensure you import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(10, 8)) # You might want to specify figure size\n",
    "    shap.summary_plot(shap_values, X_test, show=False, plot_size=(8, 6)) # show=False prevents immediate display\n",
    "    plt.title(\"SHAP Feature Importance for XGBoost Model\") # Add a title\n",
    "    plot_filename = \"shap_summary_champion.png\" # Give it a more descriptive name\n",
    "    plt.savefig(plot_filename, bbox_inches='tight', dpi=300) # Save the plot to a file\n",
    "    plt.close() # Close the plot to free memory\n",
    "\n",
    "    # 3. Log the saved plot as an MLflow artifact to the *current* active run\n",
    "    mlflow.log_artifact(plot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a464cccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully scaled.\n",
      "Shape of scaled training data: (2380, 32)\n"
     ]
    }
   ],
   "source": [
    "# --- MLP Challenger Model ---\n",
    "# Step 1: Imports and Data Scaling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Deep learning models are sensitive to feature scale. We must standardize our data.\n",
    "# We fit the scaler ONLY on the training data to prevent data leakage from the test set.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data successfully scaled.\")\n",
    "print(f\"Shape of scaled training data: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff24a49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Tensors and DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders to handle batching\n",
    "# We don't shuffle time-series data to preserve temporal order if needed, \n",
    "# but for a simple MLP, shuffling is often acceptable. Let's keep it False for rigor.\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"PyTorch Tensors and DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c556ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Model Architecture:\n",
      "ETF_MLP(\n",
      "  (layer_1): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (bn_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer_2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (bn_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define the MLP Architecture\n",
    "class ETF_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1=128, hidden_size_2=64, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the MLP model.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): The number of input features.\n",
    "            hidden_size_1 (int): Number of neurons in the first hidden layer.\n",
    "            hidden_size_2 (int): Number of neurons in the second hidden layer.\n",
    "            dropout_rate (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        super(ETF_MLP, self).__init__()\n",
    "        \n",
    "        # --- Layer Definitions ---\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.bn_1 = nn.BatchNorm1d(hidden_size_1)\n",
    "        \n",
    "        self.layer_2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.bn_2 = nn.BatchNorm1d(hidden_size_2)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size_2, 1)\n",
    "        \n",
    "        # --- Activation and Regularization ---\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" The forward pass of the model. \"\"\"\n",
    "        # First hidden layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        x = self.layer_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer with sigmoid for binary classification\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model to test\n",
    "input_features = X_train.shape[1]\n",
    "model_mlp = ETF_MLP(input_size=input_features)\n",
    "print(\"MLP Model Architecture:\")\n",
    "print(model_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a007c7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 10:02:43 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final MLP F1 Score from manual run: 0.2162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/24 10:02:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Manual MLP Training and Evaluation\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_SIZE = X_train.shape[1]\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50\n",
    "\n",
    "# --- Model, Loss, Optimizer (Demonstrates 5.2, 5.3) ---\n",
    "model_mlp = ETF_MLP(input_size=INPUT_SIZE, dropout_rate=0.4)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=LEARNING_RATE) # Adam Optimizer\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS) # LR Schedule\n",
    "\n",
    "# --- MLflow Logging ---\n",
    "with mlflow.start_run(run_name=\"MLP_Manual_Baseline\") as run:\n",
    "    mlflow.log_params({\"learning_rate\": LEARNING_RATE, \"epochs\": EPOCHS, \"optimizer\": \"Adam\"})\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(EPOCHS):\n",
    "        model_mlp.train() # Set model to training mode\n",
    "        for features, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model_mlp(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # --- Evaluation on Test Set ---\n",
    "        model_mlp.eval() # Set model to evaluation mode\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                outputs = model_mlp(features)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                all_preds.extend(predicted.numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Calculate and log F1 score for the epoch\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        mlflow.log_metric(\"test_f1_score\", f1, step=epoch)\n",
    "\n",
    "    print(f\"Final MLP F1 Score from manual run: {f1:.4f}\")\n",
    "    # Log the final model\n",
    "    mlflow.pytorch.log_model(model_mlp, \"mlp-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d750cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics and Parameters for 'ETF_Trend_Prediction' Experiment:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run Name</th>\n",
       "      <th>start_time</th>\n",
       "      <th>run_id</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>metrics.test_f1_score</th>\n",
       "      <th>params.gamma</th>\n",
       "      <th>params.colsample_bytree</th>\n",
       "      <th>params.subsample</th>\n",
       "      <th>params.n_estimators</th>\n",
       "      <th>params.max_depth</th>\n",
       "      <th>params.learning_rate</th>\n",
       "      <th>params.epochs</th>\n",
       "      <th>params.optimizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression_Baseline</td>\n",
       "      <td>2025-08-24 13:41:19.606000+00:00</td>\n",
       "      <td>f51782dd851c4b34b5c1d99a7e08b911</td>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.696839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression_Baseline</td>\n",
       "      <td>2025-08-24 13:29:53.791000+00:00</td>\n",
       "      <td>82243a7a77434ea09ed64c2f53453d49</td>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.696839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost_Tuned_Champion</td>\n",
       "      <td>2025-08-24 13:42:07.583000+00:00</td>\n",
       "      <td>e1a96194797f490e8936793b5994e0b1</td>\n",
       "      <td>0.537445</td>\n",
       "      <td>0.695210</td>\n",
       "      <td>0.491224</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.250841986049534</td>\n",
       "      <td>0.5373491740349474</td>\n",
       "      <td>0.5909528702797038</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>0.011858340455819155</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost_Tuned_Champion</td>\n",
       "      <td>2025-08-24 13:31:07.729000+00:00</td>\n",
       "      <td>e3f87602d4e8488691da91c8108c792c</td>\n",
       "      <td>0.528634</td>\n",
       "      <td>0.685756</td>\n",
       "      <td>0.488991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0343143830071153</td>\n",
       "      <td>0.6866992486209701</td>\n",
       "      <td>0.6200460582407958</td>\n",
       "      <td>103</td>\n",
       "      <td>8</td>\n",
       "      <td>0.010584233087628486</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost_Tuned_Champion</td>\n",
       "      <td>2025-08-24 13:49:07.672000+00:00</td>\n",
       "      <td>39f1168448d6461fa460df83ffb385d7</td>\n",
       "      <td>0.535242</td>\n",
       "      <td>0.662939</td>\n",
       "      <td>0.507982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.4701018940067465</td>\n",
       "      <td>0.8137350635888786</td>\n",
       "      <td>0.551705732991706</td>\n",
       "      <td>555</td>\n",
       "      <td>9</td>\n",
       "      <td>0.013846414043712146</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest_Baseline</td>\n",
       "      <td>2025-08-24 13:41:19.762000+00:00</td>\n",
       "      <td>420da46bdab94099bc200c297598b2f3</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>0.629853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForest_Baseline</td>\n",
       "      <td>2025-08-24 13:29:53.916000+00:00</td>\n",
       "      <td>1e3dc3106c6d4759be3e7e0637bb00e6</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>0.629853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost_Tuned_Champion</td>\n",
       "      <td>2025-08-24 14:02:00.535000+00:00</td>\n",
       "      <td>abda1d838b9a4c4dad3a7a41472096b2</td>\n",
       "      <td>0.522026</td>\n",
       "      <td>0.609009</td>\n",
       "      <td>0.514855</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0029757179018168</td>\n",
       "      <td>0.9562967435380642</td>\n",
       "      <td>0.7084236219872019</td>\n",
       "      <td>691</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2099188440773813</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost_Tuned_Champion</td>\n",
       "      <td>2025-08-24 14:00:18.088000+00:00</td>\n",
       "      <td>bb8466977e44478a8c41c6ff0dcd2a2d</td>\n",
       "      <td>0.524229</td>\n",
       "      <td>0.605119</td>\n",
       "      <td>0.507061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.530876810111</td>\n",
       "      <td>0.7113157460636909</td>\n",
       "      <td>0.6085537696840037</td>\n",
       "      <td>529</td>\n",
       "      <td>7</td>\n",
       "      <td>0.2708020950437727</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP_Manual_Baseline</td>\n",
       "      <td>2025-08-24 14:02:37.751000+00:00</td>\n",
       "      <td>3db6508403634aaab9ea6ab2d8645f76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50</td>\n",
       "      <td>Adam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Run Name                       start_time  \\\n",
       "0  LogisticRegression_Baseline 2025-08-24 13:41:19.606000+00:00   \n",
       "1  LogisticRegression_Baseline 2025-08-24 13:29:53.791000+00:00   \n",
       "2       XGBoost_Tuned_Champion 2025-08-24 13:42:07.583000+00:00   \n",
       "3       XGBoost_Tuned_Champion 2025-08-24 13:31:07.729000+00:00   \n",
       "4       XGBoost_Tuned_Champion 2025-08-24 13:49:07.672000+00:00   \n",
       "5        RandomForest_Baseline 2025-08-24 13:41:19.762000+00:00   \n",
       "6        RandomForest_Baseline 2025-08-24 13:29:53.916000+00:00   \n",
       "7       XGBoost_Tuned_Champion 2025-08-24 14:02:00.535000+00:00   \n",
       "8       XGBoost_Tuned_Champion 2025-08-24 14:00:18.088000+00:00   \n",
       "9          MLP_Manual_Baseline 2025-08-24 14:02:37.751000+00:00   \n",
       "\n",
       "                             run_id  Accuracy  F1 Score   ROC AUC  \\\n",
       "0  f51782dd851c4b34b5c1d99a7e08b911  0.535242  0.696839       NaN   \n",
       "1  82243a7a77434ea09ed64c2f53453d49  0.535242  0.696839       NaN   \n",
       "2  e1a96194797f490e8936793b5994e0b1  0.537445  0.695210  0.491224   \n",
       "3  e3f87602d4e8488691da91c8108c792c  0.528634  0.685756  0.488991   \n",
       "4  39f1168448d6461fa460df83ffb385d7  0.535242  0.662939  0.507982   \n",
       "5  420da46bdab94099bc200c297598b2f3  0.527533  0.629853       NaN   \n",
       "6  1e3dc3106c6d4759be3e7e0637bb00e6  0.527533  0.629853       NaN   \n",
       "7  abda1d838b9a4c4dad3a7a41472096b2  0.522026  0.609009  0.514855   \n",
       "8  bb8466977e44478a8c41c6ff0dcd2a2d  0.524229  0.605119  0.507061   \n",
       "9  3db6508403634aaab9ea6ab2d8645f76       NaN       NaN       NaN   \n",
       "\n",
       "   metrics.test_f1_score        params.gamma params.colsample_bytree  \\\n",
       "0                    NaN                None                    None   \n",
       "1                    NaN                None                    None   \n",
       "2                    NaN   4.250841986049534      0.5373491740349474   \n",
       "3                    NaN  1.0343143830071153      0.6866992486209701   \n",
       "4                    NaN  4.4701018940067465      0.8137350635888786   \n",
       "5                    NaN                None                    None   \n",
       "6                    NaN                None                    None   \n",
       "7                    NaN  1.0029757179018168      0.9562967435380642   \n",
       "8                    NaN      4.530876810111      0.7113157460636909   \n",
       "9               0.216216                None                    None   \n",
       "\n",
       "     params.subsample params.n_estimators params.max_depth  \\\n",
       "0                None                None             None   \n",
       "1                None                None             None   \n",
       "2  0.5909528702797038                 117                4   \n",
       "3  0.6200460582407958                 103                8   \n",
       "4   0.551705732991706                 555                9   \n",
       "5                None                None             None   \n",
       "6                None                None             None   \n",
       "7  0.7084236219872019                 691                5   \n",
       "8  0.6085537696840037                 529                7   \n",
       "9                None                None             None   \n",
       "\n",
       "   params.learning_rate params.epochs params.optimizer  \n",
       "0                  None          None             None  \n",
       "1                  None          None             None  \n",
       "2  0.011858340455819155          None             None  \n",
       "3  0.010584233087628486          None             None  \n",
       "4  0.013846414043712146          None             None  \n",
       "5                  None          None             None  \n",
       "6                  None          None             None  \n",
       "7    0.2099188440773813          None             None  \n",
       "8    0.2708020950437727          None             None  \n",
       "9                 0.001            50             Adam  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure MLflow is pointing to your tracking server/directory\n",
    "# mlflow.set_tracking_uri(\"../mlruns\") # Uncomment if running in a new session/script\n",
    "\n",
    "# Get the experiment by its name\n",
    "experiment = mlflow.get_experiment_by_name(\"ETF_Trend_Prediction\")\n",
    "\n",
    "if experiment:\n",
    "    # Search for all runs within this experiment\n",
    "    runs_df = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        # Order by F1 score (desc) and then by start time (desc)\n",
    "        order_by=[\"metrics.f1_score DESC\", \"start_time DESC\"],\n",
    "        output_format=\"pandas\"\n",
    "    )\n",
    "\n",
    "    # --- New Logic to Extract All Metrics and Parameters ---\n",
    "    # Identify all metric and parameter columns\n",
    "    metric_cols = [col for col in runs_df.columns if col.startswith(\"metrics.\")]\n",
    "    param_cols = [col for col in runs_df.columns if col.startswith(\"params.\")]\n",
    "\n",
    "    # Select core run info, all metrics, and all parameters\n",
    "    # The 'tags.mlflow.runName' contains the run name\n",
    "    selected_cols = [\n",
    "        \"tags.mlflow.runName\", \"start_time\", \"run_id\"\n",
    "    ] + metric_cols + param_cols\n",
    "\n",
    "    metrics_and_params = runs_df[selected_cols].copy()\n",
    "\n",
    "    # Rename columns for better readability (optional, you can keep original for params if many)\n",
    "    # This example renames just the core and metric columns\n",
    "    metrics_and_params.rename(columns={\n",
    "        \"tags.mlflow.runName\": \"Run Name\",\n",
    "        \"metrics.f1_score\": \"F1 Score\",\n",
    "        \"metrics.accuracy\": \"Accuracy\",\n",
    "        \"metrics.roc_auc\": \"ROC AUC\"\n",
    "        # Add more renames for specific metrics/params if you want,\n",
    "        # but for ALL params, it might be too many to rename individually.\n",
    "        # Keeping 'params.param_name' is often fine.\n",
    "    }, inplace=True)\n",
    "\n",
    "    print(\"Metrics and Parameters for 'ETF_Trend_Prediction' Experiment:\")\n",
    "    display(metrics_and_params)\n",
    "\n",
    "else:\n",
    "    print(f\"Experiment 'ETF_Trend_Prediction' not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
