{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import shap\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "mlflow.set_tracking_uri(\"../mlruns\")\n",
    "\n",
    "# Load your processed data\n",
    "DATA_PATH = '../data/processed/etf_features.parquet'\n",
    "data = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7dc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chronological split point\n",
    "# For example, use data up to the end of 2021 for training, and 2022 onwards for testing.\n",
    "split_date = '2022-01-01'\n",
    "X_train, X_test = X.loc[:split_date], X.loc[split_date:]\n",
    "y_train, y_test = y.loc[:split_date], y.loc[split_date:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "mlflow.set_experiment(\"ETF_Trend_Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b6db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "with mlflow.start_run(run_name=\"LogisticRegression_Baseline\"):\n",
    "    model_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    y_pred_lr = model_lr.predict(X_test)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred_lr))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred_lr))\n",
    "    print(f\"Logistic Regression F1 Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "\n",
    "# Train Random Forest\n",
    "with mlflow.start_run(run_name=\"RandomForest_Baseline\"):\n",
    "    model_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred_rf))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred_rf))\n",
    "    print(f\"Random Forest F1 Score: {f1_score(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dee337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Use TimeSeriesSplit for robust cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=tscv, scoring='f1', n_jobs=-1).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the study to find the best params\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=125) \n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best XGBoost Params:\", best_params)\n",
    "\n",
    "# Train the final XGBoost model with the best parameters and log to MLflow\n",
    "with mlflow.start_run(run_name=\"XGBoost_Tuned_Champion\") as run:\n",
    "    final_xgb_model = xgb.XGBClassifier(**best_params, random_state=42)\n",
    "    final_xgb_model.fit(X_train, y_train)\n",
    "    y_pred_xgb = final_xgb_model.predict(X_test)\n",
    "    y_pred_proba_xgb = final_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred_xgb)\n",
    "    print(f\"Final Tuned XGBoost F1 Score: {f1:.4f}\")\n",
    "\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred_xgb))\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc_score(y_test, y_pred_proba_xgb))\n",
    "\n",
    "    mlflow.xgboost.log_model(final_xgb_model, \"xgb-model\")\n",
    "    champion_run_id = run.info.run_id # Capture run ID\n",
    "\n",
    "    # --- SHAP Plot Generation and Logging (Move these lines here) ---\n",
    "    print(\"\\nSHAP analysis complete and plot logged to MLflow.\")\n",
    "\n",
    "    # 1. Create a SHAP Explainer\n",
    "    explainer = shap.TreeExplainer(final_xgb_model)\n",
    "    shap_values = explainer.shap_values(X_test) # Or X_train, depending on what you want to explain\n",
    "\n",
    "    # 2. Generate and save the SHAP summary plot to a temporary file\n",
    "    # Ensure you import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(10, 8)) # You might want to specify figure size\n",
    "    shap.summary_plot(shap_values, X_test, show=False, plot_size=(8, 6)) # show=False prevents immediate display\n",
    "    plt.title(\"SHAP Feature Importance for XGBoost Model\") # Add a title\n",
    "    plot_filename = \"shap_summary_champion.png\" # Give it a more descriptive name\n",
    "    plt.savefig(plot_filename, bbox_inches='tight', dpi=300) # Save the plot to a file\n",
    "    plt.close() # Close the plot to free memory\n",
    "\n",
    "    # 3. Log the saved plot as an MLflow artifact to the *current* active run\n",
    "    mlflow.log_artifact(plot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MLP Challenger Model ---\n",
    "# Step 1: Imports and Data Scaling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Deep learning models are sensitive to feature scale. We must standardize our data.\n",
    "# We fit the scaler ONLY on the training data to prevent data leakage from the test set.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data successfully scaled.\")\n",
    "print(f\"Shape of scaled training data: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders to handle batching\n",
    "# We don't shuffle time-series data to preserve temporal order if needed, \n",
    "# but for a simple MLP, shuffling is often acceptable. Let's keep it False for rigor.\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"PyTorch Tensors and DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c556ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the MLP Architecture\n",
    "class ETF_MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1=128, hidden_size_2=64, dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the MLP model.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): The number of input features.\n",
    "            hidden_size_1 (int): Number of neurons in the first hidden layer.\n",
    "            hidden_size_2 (int): Number of neurons in the second hidden layer.\n",
    "            dropout_rate (float): The dropout probability.\n",
    "        \"\"\"\n",
    "        super(ETF_MLP, self).__init__()\n",
    "        \n",
    "        # --- Layer Definitions ---\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.bn_1 = nn.BatchNorm1d(hidden_size_1)\n",
    "        \n",
    "        self.layer_2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.bn_2 = nn.BatchNorm1d(hidden_size_2)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_size_2, 1)\n",
    "        \n",
    "        # --- Activation and Regularization ---\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" The forward pass of the model. \"\"\"\n",
    "        # First hidden layer\n",
    "        x = self.layer_1(x)\n",
    "        x = self.bn_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        x = self.layer_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer with sigmoid for binary classification\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model to test\n",
    "input_features = X_train.shape[1]\n",
    "model_mlp = ETF_MLP(input_size=input_features)\n",
    "print(\"MLP Model Architecture:\")\n",
    "print(model_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Manual MLP Training and Evaluation\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_SIZE = X_train.shape[1]\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50\n",
    "\n",
    "# --- Model, Loss, Optimizer (Demonstrates 5.2, 5.3) ---\n",
    "model_mlp = ETF_MLP(input_size=INPUT_SIZE, dropout_rate=0.4)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=LEARNING_RATE) # Adam Optimizer\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS) # LR Schedule\n",
    "\n",
    "# --- MLflow Logging ---\n",
    "with mlflow.start_run(run_name=\"MLP_Manual_Baseline\") as run:\n",
    "    mlflow.log_params({\"learning_rate\": LEARNING_RATE, \"epochs\": EPOCHS, \"optimizer\": \"Adam\"})\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(EPOCHS):\n",
    "        model_mlp.train() # Set model to training mode\n",
    "        for features, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model_mlp(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # --- Evaluation on Test Set ---\n",
    "        model_mlp.eval() # Set model to evaluation mode\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                outputs = model_mlp(features)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                all_preds.extend(predicted.numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Calculate and log F1 score for the epoch\n",
    "        f1 = f1_score(all_labels, all_preds)\n",
    "        mlflow.log_metric(\"test_f1_score\", f1, step=epoch)\n",
    "\n",
    "    print(f\"Final MLP F1 Score from manual run: {f1:.4f}\")\n",
    "    # Log the final model\n",
    "    mlflow.pytorch.log_model(model_mlp, \"mlp-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d750cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure MLflow is pointing to your tracking server/directory\n",
    "# mlflow.set_tracking_uri(\"../mlruns\") # Uncomment if running in a new session/script\n",
    "\n",
    "# Get the experiment by its name\n",
    "experiment = mlflow.get_experiment_by_name(\"ETF_Trend_Prediction\")\n",
    "\n",
    "if experiment:\n",
    "    # Search for all runs within this experiment\n",
    "    runs_df = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        # Order by F1 score (desc) and then by start time (desc)\n",
    "        order_by=[\"metrics.f1_score DESC\", \"start_time DESC\"],\n",
    "        output_format=\"pandas\"\n",
    "    )\n",
    "\n",
    "    # --- New Logic to Extract All Metrics and Parameters ---\n",
    "    # Identify all metric and parameter columns\n",
    "    metric_cols = [col for col in runs_df.columns if col.startswith(\"metrics.\")]\n",
    "    param_cols = [col for col in runs_df.columns if col.startswith(\"params.\")]\n",
    "\n",
    "    # Select core run info, all metrics, and all parameters\n",
    "    # The 'tags.mlflow.runName' contains the run name\n",
    "    selected_cols = [\n",
    "        \"tags.mlflow.runName\", \"start_time\", \"run_id\"\n",
    "    ] + metric_cols + param_cols\n",
    "\n",
    "    metrics_and_params = runs_df[selected_cols].copy()\n",
    "\n",
    "    # Rename columns for better readability (optional, you can keep original for params if many)\n",
    "    # This example renames just the core and metric columns\n",
    "    metrics_and_params.rename(columns={\n",
    "        \"tags.mlflow.runName\": \"Run Name\",\n",
    "        \"metrics.f1_score\": \"F1 Score\",\n",
    "        \"metrics.accuracy\": \"Accuracy\",\n",
    "        \"metrics.roc_auc\": \"ROC AUC\"\n",
    "        # Add more renames for specific metrics/params if you want,\n",
    "        # but for ALL params, it might be too many to rename individually.\n",
    "        # Keeping 'params.param_name' is often fine.\n",
    "    }, inplace=True)\n",
    "\n",
    "    print(\"Metrics and Parameters for 'ETF_Trend_Prediction' Experiment:\")\n",
    "    display(metrics_and_params)\n",
    "\n",
    "else:\n",
    "    print(f\"Experiment 'ETF_Trend_Prediction' not found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
